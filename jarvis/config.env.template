# JARVIS AI Assistant Configuration Template
# Copy this file to .env and fill in your actual values

# ===========================================
# Speech-to-Text (Vosk) Configuration
# ===========================================
# Path to Vosk model directory
# Download from: https://alphacephei.com/vosk/models
# Example: wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
# Then unzip and set the path below
VOSK_MODEL_PATH=models/vosk-model-small-en-us-0.15

# ===========================================
# Large Language Model (Ollama) Configuration
# ===========================================
# Examples:
# - codegemma:7b-instruct-q5_K_M (good for coding)
# - llama3:8b (general purpose)
# - mistral:7b (balanced performance)
# - codellama:7b-instruct-q3_K_M (coding focused)
LLM_MODEL=codegemma:7b-instruct-q5_K_M

# ===========================================
# Text-to-Speech (Piper) Configuration
# ===========================================
# Model files should be placed in models/piper/
# Download from: https://rhasspy.github.io/piper-samples/
TTS_MODEL_ONNX=en_US-libritts_r-medium.onnx
TTS_MODEL_JSON=en_US-libritts_r-medium.onnx.json

# ===========================================
# Voice Activation Configuration
# ===========================================
# Wake words to detect (comma-separated)
# Examples: "jarvis", "hey jarvis,okay jarvis", "alexa,hey alexa"
WAKE_WORDS=jarvis,hey jarvis,okay jarvis

# Voice activation sensitivity (0.0 to 1.0)
# Higher values = more sensitive (more false positives)
# Lower values = less sensitive (might miss wake words)
VOICE_ACTIVATION_SENSITIVITY=0.8

# ===========================================
# CLI Output Mode Configuration
# ===========================================
# Output mode for CLI: "text" or "voice"
# - text: Print responses as text
# - voice: Speak responses via TTS (default)
# Use "jarvis text" or "jarvis voice" to change mode
OUTPUT_MODE=voice

# ===========================================
# Conversation History Configuration
# ===========================================
# Reset conversation history after each response
# - true: Reset history after each response (default)
# - false: Maintain conversation context across interactions
# Useful for maintaining context in multi-turn conversations
RESET_HISTORY_AFTER_RESPONSE=true

# ===========================================
# SuperMCP Configuration
# ===========================================
# Path to SuperMCP server (relative to jarvis/ directory)
SUPERMCP_SERVER_PATH=SuperMCP/SuperMCP.py

# Timeout for SuperMCP operations in seconds
SUPERMCP_TIMEOUT=60

# ===========================================
# Optional Ollama Configuration
# ===========================================
# Uncomment these if you want to configure Ollama behavior
# OLLAMA_NO_GPU=1
# OLLAMA_NUM_THREADS=4

# ===========================================
# Optional System Configuration
# ===========================================
# Uncomment to enable debug logging
# DEBUG=1

# ===========================================
# Installation Notes
# ===========================================
# 1. Install Ollama: https://ollama.com/
# 2. Pull your LLM model: ollama pull codegemma:7b-instruct-q5_K_M
# 3. Download Piper TTS model files to models/piper/
# 4. Download Vosk model: wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
# 5. Unzip Vosk model: unzip vosk-model-small-en-us-0.15.zip -d models/
# 6. Install dependencies: pip install vosk pyaudio
# 7. Ensure all dependencies are installed: pip install -r requirements.txt
# 8. Run JARVIS: python jarvis/main.py
